# LSTM-Transformer: Detection of AI-Generated Text

This repository implements a hybrid LSTM–Transformer model to detect AI‑generated essay text. Given a student essay, the classifier predicts whether it was human‑written or generated by a large language model, targeting real‑world academic integrity scenarios.

## Table of Contents

* [Project Structure](#project-structure)
* [Installation](#installation)
* [Usage](#usage)
* [Data & Preprocessing](#data--preprocessing)
* [Model Architecture](#model-architecture)
* [Training & Evaluation](#training--evaluation)
* [Results](#results)
* [Future Work](#future-work)
* [Files](#files)
* [Dependencies](#dependencies)
* [Contributing](#contributing)
* [License](#license)

## Project Structure

```
.
├── NLP_FInal_Code.ipynb        # Jupyter notebook with preprocessing, model definition, training, and evaluation
├── NLP-final presentation.pptx # Slides summarizing motivation, architecture, and results
├── train_essays.csv            # Labeled essay dataset (human vs AI)
└── README.md                   # This file
```

## Installation

1. Clone the repository:

   ```bash
   git clone <repository_url>
   cd <repository_folder>
   ```
2. (Optional) Create a virtual environment and activate it:

   ```bash
   python3 -m venv venv
   source venv/bin/activate        # macOS/Linux
   venv\Scripts\activate         # Windows
   ```
3. Install required packages:

   ```bash
   pip install pandas numpy tensorflow scikit-learn matplotlib
   ```

## Usage

1. Open the notebook:

   ```bash
   jupyter notebook NLP_FInal_Code.ipynb
   ```
2. Execute cells in order to:

   * Load and clean `train_essays.csv` (normalize unicode, lowercase, remove noise)
   * Tokenize via `TextVectorization` (char n‑grams, vocab size 75 000, seq length 1024)
   * Define the hybrid model: BiLSTM → TransformerBlock → Conv1D → classification head
   * Train with early stopping (patience=3) and dropout regularization
   * Evaluate on held‑out essays (compute F1, precision, recall, ROC‑AUC)
3. Adjust hyperparameters (learning rate, dropout, threshold) at the top of the notebook as needed.

## Data & Preprocessing

* **Dataset**: `train_essays.csv` containing essay texts and binary labels (`0=human`, `1=AI`).
* **Cleaning**: Unicode normalize, lowercase, separate punctuation, regex removal of non‑alphabetic noise, drop duplicates.
* **Vectorization**: Character n‑grams (3–5), vocabulary size = 75 000, fixed sequence length = 1024 tokens.

## Model Architecture

1. **Embedding**: Map 1024‑token input into 64‑dim embeddings.
2. **BiLSTM**: 32 forward + 32 backward units to capture bidirectional context.
3. **TransformerBlock**: 2 attention heads, 32‑unit feed‑forward layer, residual connections, layer normalization, dropout.
4. **Conv1D**: 128 filters, kernel size = 7, stride = 3 → global max‑pool to summarize.
5. **Classification Head**: Dense(128, ReLU) → Dropout(0.5) → Dense(1, Sigmoid).

## Training & Evaluation

* **Loss**: Binary cross‑entropy.
* **Optimizer**: Adam with a low learning rate.
* **Regularization**: Early stopping (patience=3), dropout in dense layers.
* **Metrics**:

  * Primary: F1‑score ≥ 85 % on held‑out essays
  * Secondary: False positive rate ≤ 10 %
* **Threshold Tuning**: Evaluate precision–recall tradeoff to select optimal decision threshold.

## Results

* **ROC‑AUC**: ≈ 0.92 (strong discriminative power)
* **Precision–Recall** at threshold=0.47: Precision 0.85, Recall 0.83 (optimal F1)
* **Ablation**:

  * Without `TransformerBlock`, F1 drops by \~4 %.
  * Replacing Conv1D with Dense layers reduces inference speed.

## Future Work

* Swap in a pre‑trained transformer (e.g., BERT) for richer contextual embeddings.
* Evaluate robustness on adversarial paraphrases and cross‑domain essays.
* Extend to multi‑class detection (identify specific LLM families).

## Files

* `NLP_FInal_Code.ipynb`: End‑to‑end notebook.
* `NLP-final presentation.pptx`: Slide deck.
* `train_essays.csv`: Labeled essay dataset (not included due to size).

## Dependencies

* Python 3.7+
* pandas
* numpy
* tensorflow
* scikit‑learn
* matplotlib

## Contributing

Contributions are welcome! Please open an issue or submit a pull request.

